[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HD - Wolframite",
    "section": "",
    "text": "1 gh Pages\nhttps://benjamin-asdf.github.io/hdc-wolfram/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gh Pages</span>"
    ]
  },
  {
    "objectID": "index.html#mushroom-body-models",
    "href": "index.html#mushroom-body-models",
    "title": "HD - Wolframite",
    "section": "2.1 Mushroom body models?",
    "text": "2.1 Mushroom body models?\n\nmake a HD version of mushrooom body models, compare to sparse distrubute memory, modern hopfield and transformer architectures\ntry those also as analogical reasoners",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gh Pages</span>"
    ]
  },
  {
    "objectID": "index.html#further-documentation",
    "href": "index.html#further-documentation",
    "title": "HD - Wolframite",
    "section": "5.1 Further documentation",
    "text": "5.1 Further documentation\nSee the book content menu on the left side\n\nsource: notebooks/index.clj",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gh Pages</span>"
    ]
  },
  {
    "objectID": "intro2.html",
    "href": "intro2.html",
    "title": "2  HD/VSA Intro",
    "section": "",
    "text": "2.0.1 Neural Computing à la John Von Neumann\nhttps://github.com/benjamin-asdf/hdc-wolfram/issues/4\nThis follows the paper Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors P. Kanerva 2009",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#multiply-add-permute-map",
    "href": "intro2.html#multiply-add-permute-map",
    "title": "2  HD/VSA Intro",
    "section": "2.1 Multiply-Add-Permute (MAP)",
    "text": "2.1 Multiply-Add-Permute (MAP)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#visualizing-a-hypervector",
    "href": "intro2.html#visualizing-a-hypervector",
    "title": "2  HD/VSA Intro",
    "section": "3.1 Visualizing a hypervector",
    "text": "3.1 Visualizing a hypervector\nMaking a QR code kinda thing out of a hypervector:\n\n(defn plot\n  [hd]\n  (w/ArrayPlot (w/Plus 1\n                       (w/ArrayReshape hd\n                                       [(w/Sqrt dimensions)\n                                        (w/Sqrt\n                                         dimensions)]))))\n\n\n(wh/view-no-summary\n (wl/! (w/Block [(w/= 'x (plot (seed)))] 'x)))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#normalize",
    "href": "intro2.html#normalize",
    "title": "2  HD/VSA Intro",
    "section": "3.2 Normalize",
    "text": "3.2 Normalize\nAfter superposition, the hypervector contains 0 (ties), -2 and 2 elements. Sometimes, you want to normalize the hypervector to -1 and 1. In other VSA’s, you might also thin a vector to a sparsity etc.\n\n(wl/! (w/_= 'normalize w/Sign))\n\n\nnil\n\n\n(defn normalize\n  \"Returns a normalized hypervector.\"\n  [hdv]\n  `(~'normalize ~hdv))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#similarity",
    "href": "intro2.html#similarity",
    "title": "2  HD/VSA Intro",
    "section": "3.3 Similarity",
    "text": "3.3 Similarity\nHamming distance counts the bits that are different.\n\n(wl/! (w/HammingDistance (seed) (seed)))\n\n\n5002\n\nIt is always roughly 0.5 for 2 random hypervectors.\n\nImagine picking a hypervector, then flipping a coin for each bit.\nThe differences between all other hypervectors and the picked one follow a binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#dot-similarity",
    "href": "intro2.html#dot-similarity",
    "title": "2  HD/VSA Intro",
    "section": "3.4 Dot similarity",
    "text": "3.4 Dot similarity\nI use ‘dot similarity’ for the rest of this page\n\nCounts where the vectors agree minus where they disagree\nDivide by dimensions\n-1 for the negative (opposite) vector\n~0 for unrelated vectors\n1 for the equal vector\n&gt;1 if one of the vectors has values higher than 1 and they agree\n\n\n(wl/!\n (w/_= 'similarity\n       (w/fn [a b]\n         (w/Divide (w/Dot a b) dimensions))))\n\n\nnil\n\n\n(defn similarity\n  \"Returns the dot similarity of the inputs.\n  Keeps batch dim.\"\n  [a b]\n  `(~'similarity ~a ~b))\n\n\n(map float (wl/! (similarity (seed 10) (seed))))\n\n\n(0.0184 0.0114 0.0044 0.013 0.003 -0.0168 0.0058 -0.0158 0.0034 -0.002)\n\n\n3.4.1 Experiment: picking unrelated hypervectors\n\nPick a random hypervector a\nPick 10.000 random hypervectors (n)\nFind the similarity between i = 0…n and a\n\n\n(def a (wl/! (seed)))\n\n\n(wh/view-no-summary\n (w/ListPlot\n  (w/Table\n   `(~'similarity ~a ~'(seed))\n   [10000])\n  '(-&gt;\n    AxesLabel [\"n\" \"Similarity\"])))\n\n\n\nIn fact, you can pick and pick and keep picking and the similarity will be around 0. (0.5 difference)\nThe universe will run out of time before it runs out of hypervectors.\nHyperspace is ‘seemingly infinite’, this property alone is interesting for a cognitive datatype.\n\n\n\n3.4.2 Robustness\nExperiment: TODO\n\nI can recommend Kanerva 2009 and ‘Sparse Distributed Memory’ (1998) for more.\n\n\n\n(wh/view-no-summary*\n (w/ListPlot (w/Table (similarity (seed) (seed)) 10))\n false)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#superposition",
    "href": "intro2.html#superposition",
    "title": "2  HD/VSA Intro",
    "section": "5.1 Superposition",
    "text": "5.1 Superposition\nA binary operation that returns a hypervector that is similar to its inputs. It is the collection of the inputs.\nI imagine overlapping 2 see-through pieces of paper, the outcome is the superposition.\nSince neurons have sparse activation, the superposition of 2 neuronal ensembles could just be the combined activation.\n\n(wl/! (w/_= 'superposition 'Plus))\n\n\nnil\n\n\n(defn superposition\n  \"`superposition` returns a hdv that is the multiset\n  of the input `hdvs`.\n\n  The resulting hdv is similar in equal meassure to all it's inputs.\n\n  Set membership can be tested with [[similarity]].\n\n  Like `+`, superposition is associative and commutative.\n\n  Superposition distributes over permutation.\n\n  Alias `bundle`.\n  \"\n  [& hdvs]\n  `(~'superposition ~@hdvs))\n\n\n5.1.1 Null\nThe null element of the superposition operation is a zero hypervector.\n\n(wl/!\n (w/_= 'zeros\n       (w/fn [] (w/Table 0 [dimensions]))))\n\n\nnil\n\n\n(defn zeros\n  \"Returns a zero hypervector.\n   This is the null element of the superposition operation.\"\n  []\n  (list 'zeros))\n\n\n\n5.1.2 Negative\nThe reverse of superposition is the superposition with the negative hdv (substraction).\n\n(wl/! (w/_= 'negative 'Minus))\n\n\nnil\n\n\n(defn negative\n  \"returns a hdv that is the negative of the input hdv.\"\n  [hdv]\n  `(~'negative ~hdv))\n\n\n(let [a (wl/! (seed))]\n  (wl/! (similarity a (negative a))))\n\n\n-1\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))]\n  (wl/!\n   (w/==\n    a\n    (superposition\n     (superposition a b)\n     (negative b)))))\n\n\ntrue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#bind",
    "href": "intro2.html#bind",
    "title": "2  HD/VSA Intro",
    "section": "5.2 Bind",
    "text": "5.2 Bind\n\nThe other binary operation, it returns a hypervector that is dissimilar to its inputs.\nIt is possible to recover the other input, given one input and the output.\nTherefore, ‘bind’ can be used to represent a key-value pair.\n\n\n(wl/! (w/_= 'bind 'Times))\n\n\nnil\n\n\n(defn bind\n  \"returns a hdv that is different from the inputs.\n\n  The inverse of bind is binding with the 'inverse' vector.\n  In MAP, this is the same as the original vector.\n  (In MAP, bind is the inverse of itself.)\n\n  The bind represents a `key-value-pair` of the inputs.\n\n  Bind is associative and commutative.\n  Bind distributes over superposition and permutation.\n  Bind preserves similarity.\n  \"\n  [& hdvs]\n  `(~'bind ~@hdvs))\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))\n      c (wl/! (seed))]\n  [\n   ;; Revocer input\n   (wl/! (w/== a (bind (bind a b) b)))\n\n   ;; Commutative\n   (wl/! (w/== (bind a b) (bind b a)))\n\n   ;; Associative\n   (wl/! (w/== (bind (bind a b) c) (bind a (bind b c))))\n\n   ;; Distributive over superposition\n   (wl/! (w/== (superposition (bind a c) (bind b c))\n               (bind c (superposition a b))))\n\n   ;; preserve similarity\n   ;; It is like `a`, `b` and their relationship are perfectly mirrored in the `c` domain.\n   (wl/!\n    (w/==\n     (similarity a b)\n     (similarity (bind a c) (bind b c))))])\n\n\n[true true true true true]\n\n\n5.2.1 Identity\nThe identity element of the bind operation is the identity hypervector.\nFor MAP, this is an hdv where all bits are 1.\n\n(wl/! (w/_= 'identityHdv\n            (w/fn [] (w/Table 1 [dimensions]))))\n\n\nnil\n\n\n(defn identity-hdv []\n  (list 'identityHdv))\n\nbinding a hdv with itself yields the identity hdv.\n\n(wl/! (w/== (identity-hdv) (bind a a)))\n\n\ntrue\n\nbinding a hdv with the identity hdv yields the hdv itself.\n\n(wl/! (w/== a (bind a (identity-hdv))))\n\n\ntrue\n\n{H, bind, identity-hdv} and {H, superposition, zeros} are commutative monoids.\nThe whole thing together with inverse is also a ring or something.\n\n\n5.2.2 Inverse\n\n(wl/! (w/_= 'inverse w/Identity))\n\n\nnil\n\n\n(defn inverse\n  \"`inverse` returns the inverse of the input hdv.\n\n  Bind with the inverse hdv is equivalent to unbind with the hdv.\n\n  (this is trivial in MAP, bind is it's own inverse, and a hdv is it's own inverse).\"\n  [hdv]\n  `(~'inverse ~hdv))\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))]\n  (= b (wl/! (bind (bind a b) (inverse a)))))\n\n\ntrue\n\n\n(def kvp\n  (wl/! (bind (book-keep! :a)\n              (book-keep! :b))))\n\n\n(wl/!\n (similarity\n  (book-keep! :b)\n  (bind kvp (book-keep! :a))))\n\n\n1\n\n\n(wl/!\n (similarity\n  (book-keep! :a)\n  (bind kvp (book-keep! :a))))\n\n\n81/5000\n\nBind can be thought of returning a point b ‘in the domain of’ the input a.\n\n(let [m (wl/!\n         (bind\n          (book-keep! :green-domain)\n          (superposition\n           (book-keep! :a)\n           (book-keep! :b)\n           (book-keep! :c))))]\n  [(float (wl/! (similarity m (book-keep! :c))))\n\n   ;; Ask given :c 'in the the green-domain', is it contained in m?\n   (float\n    (wl/! (similarity m\n                      (bind\n                       (book-keep! :green-domain)\n                       (book-keep! :c)))))])\n\n\n[-0.0134 0.9938]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#permute",
    "href": "intro2.html#permute",
    "title": "2  HD/VSA Intro",
    "section": "5.3 Permute",
    "text": "5.3 Permute\nshift / roll the bits of a hdv\n\n(wl/! (w/_= 'permute (w/fn [hdv n] (w/RotateLeft hdv n))))\n\n\nnil\n\n\n(defn permute\n  \"`permute` returns a hdv that is the input hdv rotated by n bits.\n  With second arg `n`, permute this number of times.\n\n  This is used to create a point from `hdv` in an unrelated domain.\n\n  - to create a quotation\n  - to use as position marker in a sequence\n  - to implement a non-commutative bind\n\n  Alias `protect` / `unprotect`.\n\n  Inverse [[permute-inverse]].\n  \"\n  ([hdv]\n   (permute hdv 1))\n  ([hdv n]\n   `(~'permute ~hdv ~n)))\n\n\n(defn permute-inverse\n  \"`permute-inverse` returns a hdv that is the input hdv rotated by -n bits.\n  See [[permute]].\n  \"\n  ([hdv]\n   (permute-inverse hdv 1))\n  ([hdv n]\n   (permute hdv (- n))))\n\nThis is sort of subtle but powerful.\nA permuted hdv is unrelated to the original hdv. We say permute is ‘randomizing’ the hdv, making it unrelated to hdvs of the input domain.\n\n(wl/! (similarity a (permute a)))\n\n\n-3/500\n\n\n(wl/! (similarity a (permute-inverse (permute a))))\n\n\n1\n\nUsing non-commutative bind, we can represent directed edges for a directed graph.\n\n(defn non-commutative-bind\n  ([a b]\n   (bind a (permute b))))\n\n\n(defn directed-edge\n  \"Given a tail and a head, return a directed edge.\"\n  [tail head]\n  (non-commutative-bind tail head))\n\n\n(defn edge-head\n  \"Given the tail and a directed edge, return the head of the edge.\"\n  [edge tail]\n  (bind edge (permute tail)))\n\n\n(defn edge-tail\n  \"Given the head and a directed edge, return the tail of the edge.\"\n  [edge head]\n  (permute-inverse (bind edge head)))\n\n\n(defn directed-graph\n  [edges]\n  (apply superposition\n         (map (fn [[tail head]] (directed-edge tail head)) edges)))\n\n\n(let [g (directed-graph [[(book-keep! :a) (book-keep! :b)]\n                         [(book-keep! :b) (book-keep! :c)]\n                         [(book-keep! :c) (book-keep! :a)]])]\n  (item-cleanup\n   (edge-tail g (book-keep! :a))))\n\n\n(:b)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  },
  {
    "objectID": "intro2.html#what-is-the-dollar-in-mexico",
    "href": "intro2.html#what-is-the-dollar-in-mexico",
    "title": "2  HD/VSA Intro",
    "section": "6.1 What is the Dollar in Mexico?",
    "text": "6.1 What is the Dollar in Mexico?\n\n(defn record [m]\n  (apply superposition\n         (map (fn [[k v]] (bind k v)) m)))\n\n\n(defn -&gt;record\n  [m]\n  (wl/! (record (map #(mapv book-keep! %) m))))\n\n\n(def usa-record\n  (-&gt;record\n   {:country :usa\n    :capital :washington\n    :currency :dollar}))\n\n\n(def mexico-record\n  (-&gt;record\n   {:capital :mexico-city\n    :country :mexico\n    :currency :peso}))\n\n\n(def query-outcome\n  (wl/!\n   ;; ~ :peso\n   (bind\n    mexico-record\n    ;; ~ :currency\n    (bind usa-record (book-keep! :dollar)))))\n\n\n(take 10 query-outcome)\n\n\n(-3 -3 -3 -3 1 3 1 -1 -3 -3)\n\n\n(map\n #(select-keys % [:obj :similarity])\n (cleanup-1 @item-memory query-outcome 0.2))\n\n\n({:obj :peso, :similarity 2573/2500})\n\n\nSuperposition enables ‘programming in superposition’.\n\n(def record-xyz (wl/!\n                 (record\n                  [[(book-keep! :x) (book-keep! 1)]\n                   [(book-keep! :y) (book-keep! 2)]\n                   [(book-keep! :z) (superposition\n                                     (book-keep! 1)\n                                     (book-keep! 2)\n                                     (book-keep! 3))]])))\n\nthe value in the :x domain is 1\n\n(item-cleanup (bind record-xyz (book-keep! :x)))\n\n\n(1)\n\nthe value in the :y domain is 2\n\n(item-cleanup (bind record-xyz (book-keep! :y)))\n\n\n(2)\n\nthe value in the :z domain is the superposition of 1,2,3\n\n(item-cleanup (bind record-xyz (book-keep! :z)))\n\n\n(1 2 3)\n\nProgramming in superposition…\n\nA collection can be treated like an element.\nEverything is a set.\nambiguity / continuity is first class.\n\n\nDot similarity explorations:\n\n(let [a (wl/! (seed))]\n  (wl/! (similarity a a)))\n\n\n1\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))\n      c (wl/! (seed))]\n  (float\n   (wl/!\n    (similarity a (w/Plus a b c)))))\n\n\n0.9924\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))\n      c (wl/! (seed))]\n  (float\n   (wl/!\n    (similarity\n     (w/Plus a b c)\n     (w/Plus a b c)))))\n\n\n2.9816\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))\n      c (wl/! (seed))]\n  (float\n   (wl/!\n    (similarity\n     (normalize (w/Plus a b c))\n     (normalize (w/Plus a b c))))))\n\n\n1.0\n\n\n(let [a (wl/! (seed))\n      b (wl/! (seed))\n      c (wl/! (seed))]\n  (float\n   (wl/!\n    (similarity\n     a\n     (normalize (w/Plus a b c))))))\n\n\n0.5\n\n\nsource: notebooks/intro2.clj",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HD/VSA Intro</span>"
    ]
  }
]